{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47312c8-4a73-4aa3-b614-7b43580fa498",
   "metadata": {},
   "source": [
    "# Preparação do ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa99ec-c912-4433-9ec0-0da4b4f6635f",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52989da2-a5e5-4fad-93a3-36f9d9462161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import re\n",
    "import unidecode\n",
    "\n",
    "from datetime import datetime\n",
    "from joblib import load, dump\n",
    "from os import cpu_count\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1d245c-a881-496a-93b6-b69967fcc60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "if os.name=='posix':\n",
    "    os.environ['PYTHONWARNINGS']='ignore'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab38bd-bfd7-4c13-9344-f2ff45111801",
   "metadata": {},
   "source": [
    "## Constantes e funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027b858a-6740-4bb1-8d01-bb0d6d0da6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_MARKETPLACES = '../datasets/samples_labeling/issues_fiscaliza/20240423/results/labels.parquet'\n",
    "FILE_SUPERVISAO_MERCADO = '../datasets/inspecao_ecommerce/supervisao_mercado.xlsx'\n",
    "\n",
    "FILE_MODEL_CLF3 = '../experimentos/assets/exp_clf3_model.joblib'\n",
    "\n",
    "FILE_HYPER_PARAMETERS_VECTORIZERS = '../datasets/experimento_gs/gs_hyper_paramenters_vectorizers.json'\n",
    "FILE_HYPER_PARAMETERS_CLASSIFIERS = '../datasets/experimento_gs/gs_hyper_paramenters_classifiers.json'\n",
    "FILE_HYPER_PARAMETERS_CLF_VECTORIZERS = '../datasets/experimento_gs/gs_hyper_paramenters_clf_vectorizer.json'\n",
    "\n",
    "N_JOBS = os.cpu_count()//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ef5c7e-08cf-4023-b088-7e38e4ac20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_mercado(file_mercado=FILE_SUPERVISAO_MERCADO):\n",
    "    dict_df_mercado = pd.read_excel(\n",
    "    file_mercado,\n",
    "    sheet_name=None,\n",
    "    usecols=[2,8,10],\n",
    "    names=['texto_busca', 'titulo', 'passivel_homologacao'],\n",
    "    true_values=['Sim', 'sim'],\n",
    "    false_values=['Não','não'],\n",
    "    na_values=['-'])\n",
    "\n",
    "    df_list = []\n",
    "    for key in dict_df_mercado.keys():\n",
    "        df = dict_df_mercado[key]\n",
    "        df['marketplace'] = key\n",
    "        df_list.append(df)\n",
    "        \n",
    "    df_mercado = pd.concat(df_list)\n",
    "    df_mercado = df_mercado.dropna()\n",
    "    df_mercado['passivel_homologacao'] = df_mercado['passivel_homologacao'].astype(int)\n",
    "    \n",
    "    map_marketplaces = {\n",
    "        'Amazon': 'Amazon', \n",
    "        'Americanas': 'Lojas Americanas',\n",
    "        'CasasBahia': 'Casas Bahia',\n",
    "        'Magalu': 'Magazine Luiza', \n",
    "        'MercadoLivre': 'Mercado Livre'\n",
    "    }\n",
    "    \n",
    "    df_mercado['marketplace'] = df_mercado['marketplace'].map(map_marketplaces)\n",
    "\n",
    "    return df_mercado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d21329-eb3a-4fcd-b06c-c4ce104d1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_params(file):\n",
    "\n",
    "    with open(file) as f:\n",
    "        clf_best_params = json.load(f)\n",
    "        \n",
    "    return pd.DataFrame(clf_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5bde4a-70f0-4d17-bb9e-3a07636d3e9d",
   "metadata": {},
   "source": [
    "# Carga e prepação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0facbb4c-6363-4a6f-9982-a1131aaa8954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marketplaces = pd.read_parquet(FILE_MARKETPLACES)\n",
    "df_mercado = load_file_mercado()\n",
    "\n",
    "# reduzir a base de dados para construir o notebook\n",
    "# df_marketplaces = df_marketplaces.sample(10)\n",
    "\n",
    "# conjunto de dados\n",
    "docs_marketplaces = df_marketplaces[df_marketplaces['passivel_homologacao']<2]['titulo'].values\n",
    "targets_marketplaces = df_marketplaces[df_marketplaces['passivel_homologacao']<2]['passivel_homologacao'].values\n",
    "\n",
    "# Split data to keep experiment results comparable\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    docs_marketplaces, targets_marketplaces,\n",
    "    test_size=0.25, \n",
    "    random_state=724\n",
    ")\n",
    "\n",
    "X_valid, y_valid = df_mercado['titulo'].values, df_mercado['passivel_homologacao'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad603cb3-32e3-40b5-80a1-42acf5b581d8",
   "metadata": {},
   "source": [
    "# Análise\n",
    "\n",
    "\n",
    "https://levity.ai/blog/text-classifiers-in-machine-learning-a-practical-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a61dbc-047d-4bb3-a87f-821a668bffae",
   "metadata": {},
   "source": [
    "## Ajustando hiperparâmetros do vetorizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aee986a0-77c4-4b0f-ae76-ee356f085a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bb446-a2ac-4786-87f5-74f3efdd92f8",
   "metadata": {},
   "source": [
    "### Pré processamento do texto\n",
    "\n",
    "Em experimentos anteriores, não extensamente documentados, observei que utilizar uma função para pré processar o texto é mais lento que processar o texto junto com o vetorizador do scikit-learn, assim, será preparada a lista de stop_words, deixando o processo de normalização (passar todas as palavras para minúsculas e remover acentos para o vetorizador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cc9a057-7902-426e-8489-ecb4d9225ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('portuguese')\n",
    "stop_words.extend(stopwords.words('english'))\n",
    "stop_words.extend(list(punctuation))\n",
    "# stopwords específicas do domínio\n",
    "stop_words.extend(['cm', 'feature', 'features', 'informações', 'itens', 'leve', 'list', 'nulo', 'package', 'pacote', 'pacotes', 'recurso', 'tamanho', 'ver', 'unidades', 'fio', 'universal'])\n",
    "# cores mais comumns\n",
    "stop_words.extend(['preto', 'cinza', 'branco', 'rosa', 'vermelho', 'laranja', 'amarelo', 'verde', 'azul', 'roxo', 'marrom'])\n",
    "# remover da lista de stopwords a palavra sem para formar o bigrama \"sem fio\", que pode ser relevante para o domínio\n",
    "# stop_words.remove('sem')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72020674-d78c-4a88-9372-d03fee3059da",
   "metadata": {},
   "source": [
    "### Vectorizer Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d7c7c-3b6e-4735-a4c7-c34137990a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2100 candidates, totalling 10500 fits\n",
      "CPU times: total: 1min 50s\n",
      "Wall time: 1h 28min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = 'passthrough'\n",
    "clf = LogisticRegression(C=10)\n",
    "pipe = Pipeline(steps = [('vectorizer',vectorizer),('clf',clf)])\n",
    "\n",
    "parameter_grid = [\n",
    "    {\n",
    "        'vectorizer': [CountVectorizer()],\n",
    "        'vectorizer__tokenizer': (None, word_tokenize),\n",
    "        'vectorizer__strip_accents': ['unicode'],\n",
    "        'vectorizer__stop_words': [None, stop_words],\n",
    "        'vectorizer__ngram_range': ((1,1),(1,2),(1,3)),\n",
    "        'vectorizer__max_df': np.linspace(0,1,5),\n",
    "        'vectorizer__min_df': np.linspace(0,1,5),\n",
    "    },\n",
    "    {\n",
    "        'vectorizer': [TfidfVectorizer()],\n",
    "        'vectorizer__tokenizer': (None, word_tokenize),\n",
    "        'vectorizer__use_idf': (True, False),\n",
    "        'vectorizer__tokenizer': (None, word_tokenize),\n",
    "        'vectorizer__strip_accents': ['unicode'],\n",
    "        'vectorizer__stop_words': [None, stop_words],\n",
    "        'vectorizer__ngram_range': ((1,1),(1,2),(1,3)),\n",
    "        'vectorizer__max_df': np.linspace(0,1,5),\n",
    "        'vectorizer__min_df': np.linspace(0,1,5),\n",
    "        'vectorizer__norm': (None, 'l1', 'l2'),\n",
    "    }\n",
    "]\n",
    "\n",
    "scoring = {\"AUC\": \"roc_auc\"}\n",
    "\n",
    "gs = GridSearchCV(pipe,parameter_grid,scoring=scoring,refit='AUC',n_jobs=N_JOBS,verbose=1)\n",
    "_=gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c70d91-af49-4cd6-a222-f3274ca80bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467afe5-1222-4345-9cf5-bcb7f017d751",
   "metadata": {},
   "source": [
    "### Vectorizer/Classifier Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dc079-b686-4245-9e88-f87278b93c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_clf = load_best_params(FILE_HYPER_PARAMETERS_CLASSIFIERS)\n",
    "# manter apenas clf com resultados melhores que o experimento 3\n",
    "df_best_clf = df_best_clf[(df_best_clf['test_auc']>0.957)&(df_best_clf['valid_auc']>0.926)]\n",
    "df_best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27b592-4eb5-4970-9794-6a4b18df88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = df_best_clf[['name','best_params']].to_dict('records')\n",
    "best_clf_params = []\n",
    "for clf in best_clf:\n",
    "    clf_params = {'clf': clf['name']}\n",
    "    for key,value in json.loads(clf['best_params']).items():\n",
    "        clf_params[key] = value\n",
    "    best_clf_params.append(clf_params)\n",
    "best_clf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716dd06-c5e2-466f-8b87-a2f44269fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.25, \n",
    "    min_df=0.0, \n",
    "    ngram_range=(1, 2),\n",
    "    strip_accents='unicode')\n",
    "\n",
    "clf = 'passthrough'\n",
    "\n",
    "pipe = Pipeline(steps = [('vectorizer',vectorizer),('clf',clf)])\n",
    "\n",
    "parameter_grid = [\n",
    "    {'clf': [NuSVC()], 'clf__kernel': ['rbf'], 'clf__nu': [0.25], 'clf__tol': [0.01]},\n",
    "    {'clf': [RandomForestClassifier()],  'clf__criterion': ['log_loss'],  'clf__max_features': ['log2'],  'clf__n_estimators': [500]},\n",
    "    {'clf': [SGDClassifier()],  'clf__alpha': [1e-05],  'clf__loss': ['log_loss'],  'clf__penalty': ['l2']},\n",
    "    {'clf': [LogisticRegression()], 'clf__C': [10.0], 'clf__penalty': ['l2']}, \n",
    "    {'clf': [LinearSVC()],  'clf__C': [1.0],  'clf__loss': ['squared_hinge'],  'clf__tol': [1.0]},\n",
    "    {'clf': [RidgeClassifier()], 'clf__solver': ['cholesky']}, \n",
    "    {'clf': [ComplementNB()], 'clf__alpha': [1.0]}, \n",
    "    {'clf': [MultinomialNB()], 'clf__alpha': [1.0]}]\n",
    "\n",
    "scoring = {\"AUC\": \"roc_auc\"}\n",
    "\n",
    "gs = GridSearchCV(pipe,parameter_grid,scoring=scoring,refit='AUC',n_jobs=N_JOBS,verbose=1)\n",
    "_=gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd64c05-1f2e-4bee-813e-046fee43da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectorizer_results = pd.DataFrame(gs.cv_results_)\n",
    "df_vectorizer_results['name'] = df_vectorizer_results['param_clf'].apply(lambda clf: clf.__class__.__name__)\n",
    "columns_to_keep = ['name', 'params', 'mean_fit_time', 'mean_test_AUC', 'rank_test_AUC']\n",
    "df_vectorizer_results = df_vectorizer_results[columns_to_keep].sort_values(by='mean_test_AUC',ascending=False)\n",
    "df_vectorizer_results = df_vectorizer_results.reset_index(drop=True)\n",
    "df_vectorizer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1d735-8bd6-4b78-8c47-ce6df64f3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_clf_final = df_best_clf[['name','test_auc']].merge(df_vectorizer_results[['name','mean_test_AUC','rank_test_AUC']],how='left')\n",
    "df_best_clf_final['previous_rank'] = df_best_clf_final.index.values+1\n",
    "\n",
    "columns_to_keep = ['name', 'test_auc', 'mean_test_AUC', 'previous_rank', 'rank_test_AUC']\n",
    "df_best_clf_final = df_best_clf_final[columns_to_keep]\n",
    "columns_to_keep = ['name', 'previous_test_auc', 'actual_test_auc', 'previous_rank', 'actual_rank']\n",
    "df_best_clf_final.columns = columns_to_keep\n",
    "\n",
    "df_best_clf_final['delta_test_auc'] = df_best_clf_final['actual_test_auc'] - df_best_clf_final['previous_test_auc']\n",
    "df_best_clf_final['delta_rank'] = df_best_clf_final['previous_rank'] - df_best_clf_final['actual_rank']\n",
    "\n",
    "df_best_clf_final = df_best_clf_final.sort_values(by='actual_test_auc',ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_best_clf_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33f8d0-d175-46f7-9903-90a12e61f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_clf_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
