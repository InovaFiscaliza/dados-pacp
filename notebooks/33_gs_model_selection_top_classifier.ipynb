{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47312c8-4a73-4aa3-b614-7b43580fa498",
   "metadata": {},
   "source": [
    "# Preparação do ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa99ec-c912-4433-9ec0-0da4b4f6635f",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52989da2-a5e5-4fad-93a3-36f9d9462161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import re\n",
    "import unidecode\n",
    "\n",
    "from datetime import datetime\n",
    "from joblib import dump\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d245c-a881-496a-93b6-b69967fcc60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab38bd-bfd7-4c13-9344-f2ff45111801",
   "metadata": {},
   "source": [
    "## Constantes e funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b858a-6740-4bb1-8d01-bb0d6d0da6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_FOLDER = '../datasets/experimento_gs/best_models/'\n",
    "FILE_MARKETPLACES = '../datasets/experimento_gs/marketplaces.parquet'\n",
    "FILE_SUPERVISAO_MERCADO = '../datasets/experimento_gs/supervisao_mercado.xlsx'\n",
    "FILE_HYPER_PARAMETERS_MODEL = '../datasets/experimento_gs/gs_hyper_paramenters_model.json'\n",
    "N_JOBS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef5c7e-08cf-4023-b088-7e38e4ac20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_mercado(file_mercado=FILE_SUPERVISAO_MERCADO):\n",
    "    dict_df_mercado = pd.read_excel(\n",
    "    file_mercado,\n",
    "    sheet_name=None,\n",
    "    usecols=[2,8,10],\n",
    "    names=['texto_busca', 'titulo', 'passivel_homologacao'],\n",
    "    true_values=['Sim', 'sim'],\n",
    "    false_values=['Não','não'],\n",
    "    na_values=['-'])\n",
    "\n",
    "    df_list = []\n",
    "    for key in dict_df_mercado.keys():\n",
    "        df = dict_df_mercado[key]\n",
    "        df['marketplace'] = key\n",
    "        df_list.append(df)\n",
    "        \n",
    "    df_mercado = pd.concat(df_list)\n",
    "    df_mercado = df_mercado.dropna()\n",
    "    df_mercado['passivel_homologacao'] = df_mercado['passivel_homologacao'].astype(int)\n",
    "    \n",
    "    map_marketplaces = {\n",
    "        'Amazon': 'Amazon', \n",
    "        'Americanas': 'Lojas Americanas',\n",
    "        'CasasBahia': 'Casas Bahia',\n",
    "        'Magalu': 'Magazine Luiza', \n",
    "        'MercadoLivre': 'Mercado Livre'\n",
    "    }\n",
    "    \n",
    "    df_mercado['marketplace'] = df_mercado['marketplace'].map(map_marketplaces)\n",
    "\n",
    "    return df_mercado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9effd19a-a16c-4caf-8b86-815772147096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_params():\n",
    "    \n",
    "    _best_params = {\n",
    "        'name': gs.best_estimator_['clf'].__class__.__name__,\n",
    "        'best_params': json.dumps(gs.best_params_),\n",
    "        'mean_fit_time': gs.cv_results_['mean_fit_time'].mean(),\n",
    "        'timestamp': datetime.now().timestamp(),\n",
    "        'train_auc': gs.best_score_,\n",
    "        'train_accuracy': gs.score(X_test,y_test),\n",
    "    }\n",
    "\n",
    "    # test accuracy and roc_auc_score\n",
    "    test_accuracy = gs.best_estimator_.score(X_test,y_test)\n",
    "    valid_accuracy = gs.best_estimator_.score(X_valid,y_valid)\n",
    "    \n",
    "    if hasattr(gs.best_estimator_['clf'],'predict_proba'):\n",
    "        y_test_score = gs.best_estimator_.predict_proba(X_test)[:,1]\n",
    "        y_valid_score = gs.best_estimator_.predict_proba(X_valid)[:,1]\n",
    "    else:\n",
    "        y_test_score = gs.best_estimator_.decision_function(X_test)\n",
    "        y_valid_score = gs.best_estimator_.decision_function(X_valid)\n",
    "        \n",
    "    test_auc = roc_auc_score(y_test,y_test_score)\n",
    "    valid_auc = roc_auc_score(y_valid,y_valid_score)\n",
    "\n",
    "    _best_params['test_auc'] = test_auc\n",
    "    _best_params['test_accuracy'] = test_accuracy\n",
    "    _best_params['valid_auc'] = valid_auc\n",
    "    _best_params['valid_accuracy'] = valid_accuracy\n",
    "\n",
    "    # confusion matrix on validation dataset\n",
    "    y_pred = gs.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    tn = int(cm[0][0])\n",
    "    fn = int(cm[1][0])\n",
    "    tp = int(cm[1][1])\n",
    "    fp = int(cm[0][1])\n",
    "    test_confusion = {\n",
    "        'tn': tn, \n",
    "        'fn': fn, \n",
    "        'tp': tp, \n",
    "        'fp': fp\n",
    "    }\n",
    "    _best_params['test_confusion'] = json.dumps(test_confusion)\n",
    "\n",
    "    # confusion matrix on validation dataset\n",
    "    y_pred = gs.predict(X_valid)\n",
    "    cm = confusion_matrix(y_valid,y_pred)\n",
    "    tn = int(cm[0][0])\n",
    "    fn = int(cm[1][0])\n",
    "    tp = int(cm[1][1])\n",
    "    fp = int(cm[0][1])\n",
    "    valid_confusion = {\n",
    "        'tn': tn, \n",
    "        'fn': fn, \n",
    "        'tp': tp, \n",
    "        'fp': fp\n",
    "    }\n",
    "    _best_params['valid_confusion'] = json.dumps(valid_confusion)\n",
    "       \n",
    "    return _best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e68ad-1290-43bf-a887-f791857c19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_estimator():\n",
    "    best_estimator_class_name = gs.best_estimator_['clf'].__class__.__name__\n",
    "    best_estimator_file_name = f'{MODELS_FOLDER}{best_estimator_class_name}.joblib'\n",
    "    dump(gs.best_estimator_,best_estimator_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5bde4a-70f0-4d17-bb9e-3a07636d3e9d",
   "metadata": {},
   "source": [
    "# Carga e prepação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c000094-be35-46ee-8686-91e5cf56fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marketplaces = pd.read_parquet(FILE_MARKETPLACES)\n",
    "df_mercado = load_file_mercado()\n",
    "\n",
    "# reduzir a base de dados para construir o notebook\n",
    "# df_marketplaces = df_marketplaces.sample(10)\n",
    "\n",
    "# conjunto de dados\n",
    "docs_marketplaces = df_marketplaces[df_marketplaces['passivel_homologacao']<2]['titulo'].values\n",
    "targets_marketplaces = df_marketplaces[df_marketplaces['passivel_homologacao']<2]['passivel_homologacao'].values\n",
    "\n",
    "# Split data to keep experiment results comparable\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    docs_marketplaces, targets_marketplaces,\n",
    "    test_size=0.25, \n",
    "    random_state=724\n",
    ")\n",
    "\n",
    "X_valid, y_valid = df_mercado['titulo'].values, df_mercado['passivel_homologacao'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad603cb3-32e3-40b5-80a1-42acf5b581d8",
   "metadata": {},
   "source": [
    "# Análise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e0542-107f-4271-9bba-66fb220534ce",
   "metadata": {},
   "source": [
    "## Seleção do melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c8409-60b5-4d86-984b-be2ad6600363",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_best_params = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b0b1f-8215-429f-82e1-adbcf3df175e",
   "metadata": {},
   "source": [
    "### Palavras mais frequentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26e5c4-2f1e-417c-9a38-137793bc0642",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('portuguese')\n",
    "stop_words.extend(stopwords.words('english'))\n",
    "stop_words.extend(list(punctuation))\n",
    "# stopwords específicas do domínio\n",
    "stop_words.extend(['cm', 'feature', 'features', 'informações', 'itens', 'leve', 'list', 'nulo', 'package', 'pacote', 'pacotes', 'recurso', 'tamanho', 'ver', 'unidades', 'fio', 'universal'])\n",
    "# cores mais comumns\n",
    "stop_words.extend(['preto', 'cinza', 'branco', 'rosa', 'vermelho', 'laranja', 'amarelo', 'verde', 'azul', 'roxo', 'marrom'])\n",
    "\n",
    "# remover da lista de stopwords a palavra sem para formar o bigrama \"sem fio\", que pode ser relevante para o domínio\n",
    "# manter em uma lista separada para avaliar se há efeito no desempenho do classificador\n",
    "stop_words_wo_sem = stop_words.copy()\n",
    "stop_words_wo_sem.remove('sem')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a469a-4d49-45e6-a743-64441c1ce0d3",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172135b-1138-4321-a534-6b896653594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode')\n",
    "clf = LogisticRegression()\n",
    "\n",
    "pipe = Pipeline(steps = [('vectorizer',vectorizer),('clf',clf)])\n",
    "\n",
    "parameter_grid = { \n",
    "    'vectorizer__max_df': [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    'vectorizer__min_df': [1, 3, 5, 10],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams or trigrams\n",
    "    'vectorizer__tokenizer': [None, word_tokenize],\n",
    "    'vectorizer__stop_words': [None, stop_words, stop_words_wo_sem],\n",
    "    # 'vectorizer__norm': ['l1', 'l2'],\n",
    "    # 'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "    # 'clf__solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "    'clf__tol': np.logspace(-5,0,6),\n",
    "    'clf__C': np.logspace(-4,4,9),\n",
    "}\n",
    "\n",
    "scoring = {\"AUC\": \"roc_auc\"}\n",
    "gs = GridSearchCV(pipe,parameter_grid,scoring=scoring,refit='AUC',n_jobs=N_JOBS,verbose=1)\n",
    "_=gs.fit(X_train, y_train)\n",
    "\n",
    "save_best_estimator()\n",
    "models_best_params.append(extract_best_params())\n",
    "df = pd.DataFrame(models_best_params)\n",
    "df = df.sort_values(by='valid_auc',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5683d5-8d1c-4b85-9114-46aae28b1780",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8044240-debc-4f69-a1c4-e9436ab16d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# clf = LinearSVC()\n",
    "\n",
    "# pipe = Pipeline(steps = [('vectorizer',vectorizer),('clf',clf)])\n",
    "\n",
    "# parameter_grid = {\n",
    "#     # 'vectorizer__max_df': (0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "#     # 'vectorizer__min_df': (1, 3, 5, 10),\n",
    "#     # 'vectorizer__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams or trigrams\n",
    "#     # 'vectorizer__tokenizer': (None, word_tokenize),\n",
    "#     'vectorizer__strip_accents': ['ascii', 'unicode'],\n",
    "#     # 'vectorizer__stop_words': [None, stop_words, stop_words_wo_sem],\n",
    "#     # 'vectorizer__norm': (\"l1\", \"l2\"),\n",
    "#     'clf__penalty': ('l1', 'l2'),\n",
    "# }\n",
    "\n",
    "# scoring = {\"AUC\": \"roc_auc\"}\n",
    "# gs = GridSearchCV(pipe,parameter_grid,scoring=scoring,refit='AUC',n_jobs=N_JOBS,verbose=1)\n",
    "# _=gs.fit(X_train, y_train)\n",
    "\n",
    "# models_best_params.append(extract_best_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f4985-91c7-4b5e-8625-04ca3e9287e2",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80e1a1-f89d-410c-8629-3c8581934519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# clf = SGDClassifier()\n",
    "\n",
    "# pipe = Pipeline(steps = [('vectorizer',vectorizer),('clf',clf)])\n",
    "\n",
    "# parameter_grid = {\n",
    "#     # 'vectorizer__max_df': (0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "#     # 'vectorizer__min_df': (1, 3, 5, 10),\n",
    "#     # 'vectorizer__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams or trigrams\n",
    "#     # 'vectorizer__tokenizer': (None, word_tokenize),\n",
    "#     'vectorizer__strip_accents': ['ascii', 'unicode'],\n",
    "#     # 'vectorizer__stop_words': [None, stop_words, stop_words_wo_sem],\n",
    "#     # 'vectorizer__norm': (\"l1\", \"l2\"),\n",
    "#     'clf__alpha': (0.5, 1),\n",
    "# }\n",
    "\n",
    "# scoring = {\"AUC\": \"roc_auc\"}\n",
    "\n",
    "# gs = GridSearchCV(pipe,parameter_grid,scoring=scoring,refit='AUC',n_jobs=N_JOBS,verbose=1)\n",
    "# _=gs.fit(X_train, y_train)\n",
    "# models_best_params.append(extract_best_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c39dc9-70a4-41fc-b286-da4497440bdb",
   "metadata": {},
   "source": [
    "### NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d92ba-3ce1-46e9-af04-e053bef650bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# clf = NuSVC()\n",
    "\n",
    "# pipe = Pipeline(steps = [('vectorizer',vectorizer),('clf',clf)])\n",
    "\n",
    "# parameter_grid = {\n",
    "#     # 'vectorizer__max_df': (0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "#     # 'vectorizer__min_df': (1, 3, 5, 10),\n",
    "#     # 'vectorizer__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams or trigrams\n",
    "#     # 'vectorizer__tokenizer': (None, word_tokenize),\n",
    "#     'vectorizer__strip_accents': ['ascii', 'unicode'],\n",
    "#     # 'vectorizer__stop_words': [None, stop_words, stop_words_wo_sem],\n",
    "#     # 'vectorizer__norm': (\"l1\", \"l2\"),\n",
    "#     'clf__alpha': (0.5, 1),\n",
    "# }\n",
    "\n",
    "# scoring = {\"AUC\": \"roc_auc\"}\n",
    "\n",
    "# gs = GridSearchCV(pipe,parameter_grid,scoring=scoring,refit='AUC',n_jobs=N_JOBS,verbose=1)\n",
    "# _=gs.fit(X_train, y_train)\n",
    "# models_best_params.append(extract_best_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262fae18-b2b7-4b20-9237-dd31f985d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models_best_params = pd.DataFrame(models_best_params)\n",
    "df_models_best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
