{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47312c8-4a73-4aa3-b614-7b43580fa498",
   "metadata": {},
   "source": [
    "# Preparação do ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa99ec-c912-4433-9ec0-0da4b4f6635f",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52989da2-a5e5-4fad-93a3-36f9d9462161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "\n",
    "# from collections import defaultdict\n",
    "from itertools import pairwise\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import punctuation\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab38bd-bfd7-4c13-9344-f2ff45111801",
   "metadata": {},
   "source": [
    "## Constantes e funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea62814d-fde8-4327-84a3-34f80b5cc317",
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL_STRING = '#NULO#'\n",
    "MAX_CAT_COUNT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0095e1b0-be3e-49b4-ba29-d5761acca67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_docs(docs,alpha=False):\n",
    "   \n",
    "    # limpeza inicial dos documentos\n",
    "    # lowercase e remoção de acentos\n",
    "    # extração e contatgem dos tokens\n",
    "    clean_tokens = []\n",
    "    tokens_freq = FreqDist()\n",
    "\n",
    "    stop_words = stopwords.words('portuguese')\n",
    "    # alguns anúncios possuem texto relacionado em inglês\n",
    "    stop_words.extend(stopwords.words('english'))\n",
    "    stop_words.extend(set(word_tokenize(NULL_STRING.lower())))\n",
    "    \n",
    "    for doc in tqdm(docs):\n",
    "        doc = unidecode.unidecode(doc.lower())\n",
    "        if not alpha:\n",
    "            tokens = [token for token in word_tokenize(doc) if token not in stop_words and token not in punctuation]\n",
    "        else:\n",
    "            tokens = [token for token in word_tokenize(doc) if token not in stop_words and token not in punctuation and token.isalpha()]\n",
    "        clean_tokens.append(tokens)\n",
    "        tokens_freq.update(tokens)\n",
    "    \n",
    "    clean_docs = [' '.join(tokens) for tokens in clean_tokens]\n",
    "\n",
    "    return clean_docs, clean_tokens, tokens_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889abb08-a685-4d8c-9d83-8c443ce91f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_doc_vectors(docs, targets, top_cats=False, s=5, ngram_range=(1,1)):\n",
    "    \n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    docs_count_matrix = count_vectorizer.fit_transform(docs)\n",
    "    \n",
    "    count_vectorizer_2 = CountVectorizer(min_df=2,ngram_range=ngram_range)\n",
    "    docs_count_matrix_2 = count_vectorizer_2.fit_transform(docs)\n",
    "    \n",
    "    count_vectorizer_5 = CountVectorizer(min_df=5,ngram_range=ngram_range)\n",
    "    docs_count_matrix_5 = count_vectorizer_5.fit_transform(docs)\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "    docs_tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "    \n",
    "    tfidf_vectorizer_2 = TfidfVectorizer(min_df=2,ngram_range=ngram_range)\n",
    "    docs_tfidf_matrix_2 = tfidf_vectorizer_2.fit_transform(docs)\n",
    "    \n",
    "    tfidf_vectorizer_5 = TfidfVectorizer(min_df=5,ngram_range=ngram_range)\n",
    "    docs_tfidf_matrix_5 = tfidf_vectorizer_5.fit_transform(docs)\n",
    "    \n",
    "    pca = TruncatedSVD(n_components=2)\n",
    "    \n",
    "    pca_count_docs = pca.fit_transform(docs_count_matrix)\n",
    "    pca_count_docs_2 = pca.fit_transform(docs_count_matrix_2)\n",
    "    pca_count_docs_5 = pca.fit_transform(docs_count_matrix_5)\n",
    "    \n",
    "    pca_tfidf_docs = pca.fit_transform(docs_tfidf_matrix)\n",
    "    pca_tfidf_docs_2 = pca.fit_transform(docs_tfidf_matrix_2)\n",
    "    pca_tfidf_docs_5 = pca.fit_transform(docs_tfidf_matrix_5)\n",
    "    \n",
    "    count_scatter_x = pca_count_docs[:, 0] # first principle component\n",
    "    count_scatter_y = pca_count_docs[:, 1] # second principle component\n",
    "    \n",
    "    count_scatter_x_2 = pca_count_docs_2[:, 0] # first principle component\n",
    "    count_scatter_y_2 = pca_count_docs_2[:, 1] # second principle component\n",
    "    \n",
    "    count_scatter_x_5 = pca_count_docs_5[:, 0] # first principle component\n",
    "    count_scatter_y_5 = pca_count_docs_5[:, 1] # second principle component\n",
    "\n",
    "    tfidf_scatter_x = pca_tfidf_docs[:, 0] # first principle component\n",
    "    tfidf_scatter_y = pca_tfidf_docs[:, 1] # second principle component\n",
    "    \n",
    "    tfidf_scatter_x_2 = pca_tfidf_docs_2[:, 0] # first principle component\n",
    "    tfidf_scatter_y_2 = pca_tfidf_docs_2[:, 1] # second principle component\n",
    "    \n",
    "    tfidf_scatter_x_5 = pca_tfidf_docs_5[:, 0] # first principle component\n",
    "    tfidf_scatter_y_5 = pca_tfidf_docs_5[:, 1] # second principle component\n",
    "    \n",
    "    fig, axs = plt.subplots(2,3,figsize=(15,10))\n",
    "    \n",
    "    # group by clusters and scatter plot every cluster\n",
    "    # with a colour and a label\n",
    "    if not top_cats:\n",
    "        for group in np.unique(targets):\n",
    "            ix = np.where(targets == group)\n",
    "            \n",
    "            axs[0][0].scatter(count_scatter_x[ix], count_scatter_y[ix], label=group, s=s)\n",
    "            axs[0][1].scatter(count_scatter_x_2[ix], count_scatter_y_2[ix], label=group, s=s)\n",
    "            axs[0][2].scatter(count_scatter_x_5[ix], count_scatter_y_5[ix], label=group, s=s)\n",
    "            \n",
    "            axs[1][0].scatter(tfidf_scatter_x[ix], tfidf_scatter_y[ix], label=group, s=s)\n",
    "            axs[1][1].scatter(tfidf_scatter_x_2[ix], tfidf_scatter_y_2[ix], label=group, s=s)\n",
    "            axs[1][2].scatter(tfidf_scatter_x_5[ix], tfidf_scatter_y_5[ix], label=group, s=s)\n",
    "    else:\n",
    "        ix_top = np.where(targets == 1)\n",
    "        ix_other = np.where(targets > 1)\n",
    "        for ix,group in zip([ix_top, ix_other],['top','other']):\n",
    "            \n",
    "            axs[0][0].scatter(count_scatter_x[ix], count_scatter_y[ix], label=group, s=s)\n",
    "            axs[0][1].scatter(count_scatter_x_2[ix], count_scatter_y_2[ix], label=group, s=s)\n",
    "            axs[0][2].scatter(count_scatter_x_5[ix], count_scatter_y_5[ix], label=group, s=s)\n",
    "            \n",
    "            axs[1][0].scatter(tfidf_scatter_x[ix], tfidf_scatter_y[ix], label=group, s=s)\n",
    "            axs[1][1].scatter(tfidf_scatter_x_2[ix], tfidf_scatter_y_2[ix], label=group, s=s)\n",
    "            axs[1][2].scatter(tfidf_scatter_x_5[ix], tfidf_scatter_y_5[ix], label=group, s=s)\n",
    "    \n",
    "    axs[0][0].set_title('CountVectorizer')\n",
    "    axs[0][1].set_title('CountVectorizer (min 2)')\n",
    "    axs[0][2].set_title('CountVectorizer (min 5)')\n",
    "    \n",
    "    axs[1][0].set_title('TfidfVectorizer')\n",
    "    axs[1][1].set_title('TfidfVectorizer (min 2)')\n",
    "    axs[1][2].set_title('TfidfVectorizer (min 5)')\n",
    "    \n",
    "    for ax in axs.flat:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # plt.xlabel(\"PCA 0\")\n",
    "    # plt.ylabel(\"PCA 1\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5bde4a-70f0-4d17-bb9e-3a07636d3e9d",
   "metadata": {},
   "source": [
    "# Carga e prepação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c000094-be35-46ee-8686-91e5cf56fc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>descricao</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Filtro de Lente de Efeitos Especiais Blue Stre...</td>\n",
       "      <td>Filtro de Lente de Efeitos Especiais Blue Stre...</td>\n",
       "      <td>Eletrônicos e Tecnologia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Motores Sem Escova, 4 Conjuntos de Motores Sem...</td>\n",
       "      <td>Motores Sem Escova, 4 Conjuntos de Motores Sem...</td>\n",
       "      <td>Eletrônicos e Tecnologia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bolsa de Armazenamento Protetora para Drone, C...</td>\n",
       "      <td>Bolsa de Armazenamento Protetora para Drone, C...</td>\n",
       "      <td>Brinquedos e Jogos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EQUSS Para DUCATI DL650 DL1000 V-strom DL 650 ...</td>\n",
       "      <td>EQUSS Para DUCATI DL650 DL1000 V-strom DL 650 ...</td>\n",
       "      <td>Eletrônicos e Tecnologia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remote control car brushless motor servo kit,r...</td>\n",
       "      <td>remote control car brushless motor servo kit,r...</td>\n",
       "      <td>Eletrônicos e Tecnologia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                nome  \\\n",
       "0  Filtro de Lente de Efeitos Especiais Blue Stre...   \n",
       "1  Motores Sem Escova, 4 Conjuntos de Motores Sem...   \n",
       "2  Bolsa de Armazenamento Protetora para Drone, C...   \n",
       "3  EQUSS Para DUCATI DL650 DL1000 V-strom DL 650 ...   \n",
       "4  remote control car brushless motor servo kit,r...   \n",
       "\n",
       "                                           descricao                    target  \n",
       "0  Filtro de Lente de Efeitos Especiais Blue Stre...  Eletrônicos e Tecnologia  \n",
       "1  Motores Sem Escova, 4 Conjuntos de Motores Sem...  Eletrônicos e Tecnologia  \n",
       "2  Bolsa de Armazenamento Protetora para Drone, C...        Brinquedos e Jogos  \n",
       "3  EQUSS Para DUCATI DL650 DL1000 V-strom DL 650 ...  Eletrônicos e Tecnologia  \n",
       "4  remote control car brushless motor servo kit,r...  Eletrônicos e Tecnologia  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_amazon = '../datasets/amazon.parquet.gzip'\n",
    "\n",
    "df_amazon = pd.read_parquet(file_amazon).reset_index(drop=True)\n",
    "\n",
    "columns_to_keep = ['Nome', 'Marca', 'Modelo', 'Descrição', 'Descrição_Principal', 'cat_0']\n",
    "df_amazon = df_amazon[columns_to_keep].fillna(NULL_STRING)\n",
    "\n",
    "columns_to_join = ['Nome', 'Marca', 'Modelo', 'Descrição', 'Descrição_Principal']\n",
    "df_amazon['descricao'] = df_amazon[columns_to_join].apply(lambda row: ' '.join(row),axis=1)\n",
    "\n",
    "columns_to_keep = ['Nome', 'descricao', 'cat_0']\n",
    "df_amazon = df_amazon[columns_to_keep]\n",
    "df_amazon.columns = ['nome', 'descricao', 'target']\n",
    "\n",
    "df_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21dd1d35-5928-47ba-9292-91f69eaff593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59f836466ba4d7396b4465375fd72e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f12bb1a43b45a2a5a0baf620a4d475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nome, tokens_nome, tf_nome = get_clean_docs(df_amazon.nome)\n",
    "descricao, tokens_descricao, tf_descicao = get_clean_docs(df_amazon.descricao)\n",
    "target = df_amazon.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c60a6c36-9b31-4cd1-8e36-62932253deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_clean = pd.DataFrame({'nome': nome, 'descricao': descricao, 'target': target})\n",
    "df_amazon_clean.to_parquet('../datasets/amazon_clean.parquet',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639f85d-f980-4863-adda-16332a3faaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_nome = [len(doc) for doc in tokens_nome]\n",
    "len_desc = [len(doc) for doc in tokens_descricao]\n",
    "\n",
    "bins_nome = [1,10,15,20,25,30,100]\n",
    "bins_desc = [1,10,15,20,50,100,1200]\n",
    "heights_nome,_ = np.histogram(len_nome,bins=bins_nome)\n",
    "heights_desc,_ = np.histogram(len_desc,bins=bins_desc)\n",
    "\n",
    "x_nome = []\n",
    "for start,stop in pairwise(bins_nome):\n",
    "    if stop==bins_nome[-1]:\n",
    "        x_nome.append(f'>={start}')\n",
    "    elif stop==start+1:\n",
    "        x_nome.append(f'{start}')\n",
    "    else:\n",
    "        x_nome.append(f'{start}-{stop-1}')\n",
    "\n",
    "x_desc = []\n",
    "for start,stop in pairwise(bins_desc):\n",
    "    if stop==bins_desc[-1]:\n",
    "        x_desc.append(f'>={start}')\n",
    "    elif stop==start+1:\n",
    "        x_desc.append(f'{start}')\n",
    "    else:\n",
    "        x_desc.append(f'{start}-{stop-1}')\n",
    "        \n",
    "fig, axs = plt.subplots(1,2,figsize=(15,5),sharey=True)\n",
    "    \n",
    "for i in range(len(heights_nome)):\n",
    "    p_nome = axs[0].bar(x_nome[i],heights_nome[i])\n",
    "    labels = ['{:,d}'.format(heights_nome[i]).replace(',','.')]\n",
    "    axs[0].bar_label(p_nome, label_type='edge', labels=labels)\n",
    "    \n",
    "for i in range(len(heights_desc)):\n",
    "    p_desc = axs[1].bar(x_desc[i],heights_desc[i])\n",
    "    labels = ['{:,d}'.format(heights_desc[i]).replace(',','.')]\n",
    "    axs[1].bar_label(p_desc, label_type='edge',labels=labels)\n",
    "    \n",
    "\n",
    "# # remove the frame of the chart\n",
    "for ax in axs:\n",
    "    \n",
    "    ax.tick_params(bottom=False, left=False,\n",
    "               labelbottom=True, labelleft=False)\n",
    "    # ax.spines['left'].set_visible(False)\n",
    "    # ax.spines['right'].set_visible(False)\n",
    "    # ax.spines['top'].set_visible(False)\n",
    "    # ax.spines['bottom'].set_visible(False)\n",
    "    \n",
    "    ax.set_xlabel('Tamanho do documento')\n",
    "    ax.set_ylabel('Quantidade de documentos')\n",
    "\n",
    "fig.suptitle('Quantidade de documentos por tamanho (quantidade de tokens)')\n",
    "axs[0].set_title('Tamanho do Nome')\n",
    "axs[1].set_title('Tamanho do Nome+Descrição')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad603cb3-32e3-40b5-80a1-42acf5b581d8",
   "metadata": {},
   "source": [
    "# Visualização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d4cd0-2b72-409c-b355-8a8b55d7ef23",
   "metadata": {},
   "source": [
    "## Nome 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f71a9-37de-40ea-84a5-05890d70e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_doc_vectors(nome,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7024be5-0aad-4973-952f-ab9acdcbb35c",
   "metadata": {},
   "source": [
    "## Nome+Descrição 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe36095-ddd2-404d-8f2a-e35faa51ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_doc_vectors(descricao,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50e4dc-24da-416c-9824-4dfeb5d26fa7",
   "metadata": {},
   "source": [
    "## Nome 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258d39c-679a-4c2f-acb9-bfea19b88a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_doc_vectors(nome,target,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc7c045-acc0-405b-a175-6b3009700bb4",
   "metadata": {},
   "source": [
    "## Nome+Descrição 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ffbdd-1708-4dcb-b8a7-d26c2803ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_doc_vectors(descricao,target,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5868f2-b4fb-4c56-a489-128e916b8a4e",
   "metadata": {},
   "source": [
    "## Nome 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c0e158-6ef4-490d-9397-bc62d5fddc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_doc_vectors(nome,target,ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d7702-5766-41f7-8582-f2547e04ed7a",
   "metadata": {},
   "source": [
    "## Nome+Descrição 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7aeda8-6ec3-476b-883b-104b9b710cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_doc_vectors(descricao,target,ngram_range=(1,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
